{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "565ab88e-88e1-40c6-97e9-a04aac283510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6267c407-1363-4b56-b7bc-bd29946cd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image pre-processing helper class\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, batches=32, in_channels=3, patch_size=16, size=128, embed_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batches = batches \n",
    "        self.in_channels = in_channels # rgb ==> 3 channels\n",
    "        self.patch_size = patch_size # size of each patch (like a token)\n",
    "        self.embed_dim = embed_dim # the higher-dimensional space to project the patches to\n",
    "        self.size = size # size of input image\n",
    "        self.N = (self.size // self.patch_size) ** 2 # number of patches\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_channels=self.in_channels, # B, C, H, W --> B, D, H_p, W_p\n",
    "                              out_channels=self.embed_dim, # 3D space --> 768D space to extract more information\n",
    "                              kernel_size=self.patch_size, # so that the patches don't overlap\n",
    "                              stride=self.patch_size) # divides input image into patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim)) # token which captures the 'meaning' of the image in a vector\n",
    "        self.pos_embeddings = nn.Parameter(torch.randn(1, self.N + 1, self.embed_dim)) # the positional embeddings which will be added later\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x) # applying conv2d projection\n",
    "        x = torch.flatten(x, 2) # B, D, N\n",
    "        x = torch.transpose(1, 2) # B, N, D\n",
    "        B = x.shape[0]\n",
    "        cls_token = self.cls_token.expand(B, -1, -1) # expanding the cls token along the batch dimension so it can be added later\n",
    "        x = torch.cat((cls_token, x), dim=1) # adding the cls token to the input tensor \n",
    "        x = x + self.pos_embeddings # now each vector is aware of the position of the word\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b3c2889-59d7-4ac7-b352-0f272ed9dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, heads=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_dim // heads\n",
    "        \n",
    "        # fully connected NN layers with # of input neurons = embed_dim = # output neurons\n",
    "        self.Q_proj = nn.Linear(embed_dim, embed_dim) \n",
    "        self.V_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.K_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.output = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        # sending the input tensor through proj NN layers\n",
    "        Q = self.Q_proj(x) # batches, patches, embed_dim\n",
    "        Q = Q.view(B, N, self.heads, self.head_dim).permute(0, 2, 1, 3) # single head --> multihead\n",
    "        \n",
    "        V = self.V_proj(x) # batches, patches, embed_dim\n",
    "        V = V.view(B, N, self.heads, self.head_dim).permute(0, 2, 1, 3) # single head --> multihead\n",
    "        \n",
    "        K = self.K_proj(x) # batches, patches, embed_dim\n",
    "        K = K.view(B, N, self.heads, self.head_dim).permute(0, 2, 1, 3) # single head --> multihead\n",
    "        \n",
    "        # computing attention\n",
    "        x = self.compute_attention(Q, K, V).permute(0, 2, 1, 3).contiguous() # B, heads, N, head_dim --> B, N, heads, head_dim\n",
    "        x = x.view(B, N, D)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def compute_attention(self, Q, K, V):\n",
    "        K_T = torch.transpose(K, -2, -1) # transpose so that multiplication is defined\n",
    "        scaling = sqrt(self.head_dim) \n",
    "        val = torch.matmul(Q, K_T) / scaling\n",
    "        \n",
    "        return torch.matmul(torch.softmax(val, dim=-1), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8be6a2fc-27c8-45f6-8b4c-75297d9971bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, heads=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mhsa = ManualMultiHeadSelfAttention(embed_dim, heads)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.mhsa(x) # computing residual\n",
    "        x = x + attn_out # skip connection\n",
    "        x = self.ln1(x) # normalizing activation functions to prevent saturation\n",
    "        ffn_out = self.ffn(x) # computing residual\n",
    "        x = x + ffn_out # skip connection\n",
    "        x = self.ln2(x) # normalizing activation functions to prevent saturation\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f711427-9e8f-4437-8fd9-feadc6890e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,  batches=32, in_channels=3, patch_size=16, size=128, embed_dim=768, heads=12, depth=8, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(batches, in_channels, patch_size, size, embed_dim)\n",
    "        self.transformer_stack = nn.ModuleList(TransformerEncoder(embed_dim, heads) for _ in range(depth))\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes),\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        for t in self.transformer_stack:\n",
    "            x = t(x)\n",
    "\n",
    "        cls = x[:, 0]\n",
    "        x = self.mlp_head(cls)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8f13ca3-8237-4f0c-8109-873b0c114953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc23c0b0-7333-488c-babc-1f009832ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.CIFAR100(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.CIFAR100(root=\"data\", train=False, download=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d1ba795-5d04-4031-b1f6-400d6da2e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "channels = 3\n",
    "patchsize = 4\n",
    "size = 32\n",
    "embeddim = 128\n",
    "numheads = 12\n",
    "encoders = 16\n",
    "numclasses = 100\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3b1f80-eda0-44c8-8a27-b7666e7dea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=training_data, batch_size=batchsize, shuffle=True)\n",
    "test_dataloder = DataLoader(dataset=test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d2763bf-5b3e-4c40-9d37-41d6461d4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(\n",
    "    batches=batchsize, in_channels=channels, \n",
    "    patch_size=patchsize, embed_dim=embeddim, \n",
    "    heads=numheads, depth=encoders, \n",
    "    num_classes=numclasses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de500a70-80c6-4abc-bdd4-20b9bb5d0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(vit.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e467e-c225-4c15-b9fa-86b16e226061",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    vit.train(True)\n",
    "    loss_total = 0\n",
    "    \n",
    "    for data in train_dataloader:\n",
    "        input, labels = data\n",
    "        optim.zero_grad()\n",
    "        output = vit(input)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    print(f\"Epoch {i + 1}: \\t Loss: {loss_total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
