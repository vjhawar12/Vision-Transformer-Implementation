{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565ab88e-88e1-40c6-97e9-a04aac283510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6267c407-1363-4b56-b7bc-bd29946cd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image pre-processing helper class\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, batches=32, in_channels=3, patch_size=16, size=128, embed_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batches = batches \n",
    "        self.in_channels = in_channels # rgb ==> 3 channels\n",
    "        self.patch_size = patch_size # size of each patch (like a token)\n",
    "        self.embed_dim = embed_dim # the higher-dimensional space to project the patches to\n",
    "        self.size = size # size of input image\n",
    "        self.N = (self.size // self.patch_size) ** 2 # number of patches\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_channels=self.in_channels, # B, C, H, W --> B, D, H_p, W_p\n",
    "                              out_channels=self.embed_dim, # 3D space --> 768D space to extract more information\n",
    "                              kernel_size=self.patch_size, # so that the patches don't overlap\n",
    "                              stride=self.patch_size) # divides input image into patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim)) # token which captures the 'meaning' of the image in a vector\n",
    "        self.pos_embeddings = nn.Parameter(torch.randn(1, self.N + 1, self.embed_dim)) # the positional embeddings which will be added later\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x) # applying conv2d projection\n",
    "        x = torch.flatten(x, 2) # B, D, N\n",
    "        x = torch.transpose(1, 2) # B, N, D\n",
    "        B = x.shape[0]\n",
    "        cls_token = self.cls_token.expand(B, -1, -1) # expanding the cls token along the batch dimension so it can be added later\n",
    "        x = torch.cat((cls_token, x), dim=1) # adding the cls token to the input tensor \n",
    "        x = x + self.pos_embeddings # now each vector is aware of the position of the word\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b3c2889-59d7-4ac7-b352-0f272ed9dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, heads=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_dim // heads\n",
    "        \n",
    "        # fully connected NN layers with # of input neurons = embed_dim = # output neurons\n",
    "        self.Q_proj = nn.Linear(embed_dim, embed_dim) \n",
    "        self.V_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.K_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.output = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        # sending the input tensor through proj NN layers\n",
    "        Q = self.Q_proj(x) # batches, patches, embed_dim\n",
    "        Q = Q.view(B, N, self.heads, self.head_dim).permute(0, 2, 1, 3) # single head --> multihead\n",
    "        \n",
    "        V = self.V_proj(x) # batches, patches, embed_dim\n",
    "        V = V.view(B, N, self.heads, self.head_dim).permute(0, 2, 1, 3) # single head --> multihead\n",
    "        \n",
    "        K = self.K_proj(x) # batches, patches, embed_dim\n",
    "        K = K.view(B, N, self.heads, self.head_dim).permute(0, 2, 1, 3) # single head --> multihead\n",
    "        \n",
    "        # computing attention\n",
    "        x = self.compute_attention(Q, K, V).permute(0, 2, 1, 3).contiguous() # B, heads, N, head_dim --> B, N, heads, head_dim\n",
    "        x = x.view(B, N, D)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def compute_attention(self, Q, K, V):\n",
    "        K_T = torch.transpose(K, -2, -1) # transpose so that multiplication is defined\n",
    "        scaling = sqrt(self.head_dim) \n",
    "        val = torch.matmul(Q, K_T) / scaling\n",
    "        \n",
    "        return torch.matmul(torch.softmax(val, dim=-1), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8be6a2fc-27c8-45f6-8b4c-75297d9971bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, heads=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mhsa = ManualMultiHeadSelfAttention(embed_dim, heads)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out = self.mhsa(x) # computing residual\n",
    "        x = x + attn_out # skip connection\n",
    "        x = self.ln1(x) # normalizing activation functions to prevent saturation\n",
    "        ffn_out = self.ffn(x) # computing residual\n",
    "        x = x + ffn_out # skip connection\n",
    "        x = self.ln2(x) # normalizing activation functions to prevent saturation\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f711427-9e8f-4437-8fd9-feadc6890e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
