# Vision Transformer (ViT) - From Scratch

## üß†  Motivation: Why Vision Transformers?

Convolutional Neural Networks (CNNs) have long been the dominant architecture for image classification. They are not only highly data-efficient and lightweight‚Äîconsider MobileNet or EfficientNet‚Äîbut also highly accurate: for example, ResNet-50 achieves around 95% accuracy on CIFAR-10.

However, the introduction of Vision Transformers (ViTs) brought a new way of thinking to the field‚Äîone that applies the attention mechanism first proposed in the 2017 paper ‚ÄúAttention Is All You Need.‚Äù

I took on this project to deepen my understanding of attention-based models and explore how these techniques can be applied beyond text data, specifically to vision tasks like image classification.

## üîç Overview

This project is a PyTorch-based implementation of the paper ‚ÄúAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.‚Äù It builds the Vision Transformer (ViT) architecture from scratch, including:

**Patch embeddings**

**Multi-head self-attention**

**Transformer encoders**

The training pipeline also integrates:

**Regularization: stochastic depth, dropout**

**Data augmentation: CutMix, MixUp, horizontal flip, and color jitter**

The model is trained and evaluated on the CIFAR-10 dataset, achieving a final test accuracy of **85.7%**.

## üõ†Ô∏è Dependency installation
Clone the repo:

```bash
git clone https://github.com/vjhawar12/Vision-Transformer-paper-implementation.git
cd Vision-Transformer-paper-implementation
```

Install the dependencies:

```bash
pip install -r requirements.txt
```

## üöÄ Running the Notebook

Option A: Open the notebook:

```bash
jupyter notebook Vision_Transformer_from_scratch.ipynb
```

Option B: Use Google Colab.

          Run all cells sequentially.
